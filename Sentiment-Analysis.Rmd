---
title: "Sentiment Analysis (with likely pre-workshop typos)"
author: "Dave Campbell"
date: "05/24/2022"
---

Note that I am using R>4.1 which gives me access to the **|>** operator.  For reproducibility this is the R version that I am using.
```{r}
version
```

All the libraries used in this document.
```{r}
#install.packages("tidyverse")
library(tidyverse)
#install.packages("tidytext")
library(tidytext)
#install.packages("dplyr")
library(dplyr)
#install.packages("stringr")
library(stringr)
#install.packages("janitor")
library(janitor)

```

# Sentiment Analysis Typical Strategies
- **Easy**: Use a lexicon of positive / negative words and count occurence within each article. This is quick and easy, but it is hard to get a lexicon that is really tuned to your goals and era.  By default this ignores context ( _costs *fell*_ "+", _profits *fell*_ "-"), and modifiers ( _my French comprehension is *not bad*_ "+", _my French spelling is *not good*_ "-").
- **Medium**: Modify a lexicon by editing some terms, looking for modifiers and/or merging terms that could be considered a single word "Data_Science".  This might involve searching for modifiers, such as *not* within a few words of *bad*.  This requires some hands on effort and domain expertise.  You'll need to read some of the text.  Note that pre-processing typically makes a lot of improvements but almost always introduces some undesirable effects.  This is equivalent to manually adding some structure to the model.
- **Bit Harder**: Label some sentences as positive / negative and build a model to figure out what it is about the sentences that define their sentiment.  Often start from something like a pre-trained *BERT* model to convert the text to numbers then use a regression-type model (neural net or boosting?) to predict sentiment.  Typically you need a lot of data. This is much better for paying attention to context, but costs more effort (labelling data and training the model).  Relies on having a well-trained (possibly domain specific) *BERT* model and that won't even be explained until later.


# Obtain Data Source.  Consider this carefully

Maybe from here:
```{r}
# https://data.world/crowdflower/economic-news-article-tone
# data is licenced as 'public domain'
news_data <- read.csv("https://query.data.world/s/irm45kkldlexxiphchk2yena6rllpn", header=TRUE, stringsAsFactors=FALSE) |> 
   as_tibble() |> 
   select(text)
# look at what is included:
news_data |> glimpse()


```

OR here:
Take a look at using Bank of Canada speeches.  The [robots](https://www.bankofcanada.ca/robots.txt) file looks good.  The other place to look for permission is in the webpage [terms and conditions](https://www.bankofcanada.ca/terms/).   We can use Bank of Canada speeches as long as we provide them free of charge (or otherwise obtain the Bank's permission) and attribute the Bank of Canada as the source.  We must also _exercise due diligence in ensuring the accuracy of any content_.:

Let's take the latest speech from Governor Tiff Macklem and split it into sentences.


```{r}
url = "https://www.bankofcanada.ca/2022/04/opening-statement-2022-04-13/"

#extract speeches.


```

Note that for the sake of time I'm glossing over several steps.

# Basic workflow

- get text data
- split it into the units of observation.  Sometimes these are documents, Tweets, or sentences.
- tokenize the observations into subunits of interest, typically these are words.
- Analyze!


# Lexicon

Let's consider lexicons!

Lexicons are a list of words with an associated sentiment.

The idea is to  match terms to a sentiment lexicon.  These have been built for a specific purpose, but hopefully such a sentiment will be useful for us.

There is no way of separating out data cleaning from data analysis.

Sentiment Analysis is all about comparing words in a document to words in a sentiment list. The best is to build your own lexicon that suits your needs, but that's expensive and slow. There are 3 popular lexicons:

- AFINN from Finn Ã…rup Nielsen,
- bing from Bing Liu and collaborators, and
- nrc from Saif Mohammad and Peter Turney at the National Research Council of Canada.
- Loughran-McDonald Financial dictionary;  

_AFINN_ gives words a score between -5 and +5 rating its severity of positive or negative sentiment.
```{r}
tidytext::get_sentiments("afinn")
hist(get_sentiments("afinn")$value)
```

_bing_ gives a binary value of positive or negative. Neutral words are not in the list.
```{r}
tidytext::get_sentiments("bing")
table(get_sentiments("bing")$sentiment)
```

_nrc_ puts each word into a sentiment category.
```{r}
tidytext::get_sentiments("nrc")
table(get_sentiments("nrc")$sentiment)
```



Lexicon of negative, positive, uncertainty, litigious, strong modal, weak modal, and constraining terms from the Loughran-McDonald financial dictionary see [here](https://sraf.nd.edu/loughranmcdonald-master-dictionary/) and the research paper
_Loughran and McDonald (2011) When is a Liability not a Liability? Textual Analysis, Dictionaries, and 10-Ks, Journal of Finance, 66:1, 35-65_

As a bonus, the dataset also includes the number of syllables.

```{r}
LM_lex = read.csv("https://drive.google.com/file/d/17CmUZM9hGUdGYjCXcjQLyybjTrcjrhik/view?usp=sharing", header = TRUE, stringsAsFactors=FALSE) 

# or the cheap version:
lapply(SentimentAnalysis::DictionaryLM,head)
lapply(SentimentAnalysis::DictionaryLM,length)

```

```

Note that each lexicon has a different length and words evolve in meaning over time.  Sick used to be bad, then it was good, now it's so bad that we stay home most of the time.



Back to sentiments, let's count the _nrc_ category occurences:

```{r}
Lyrics1 = LyricsJB1 %>% mutate(album= "My world")
Lyrics2 = LyricsJB2 %>% mutate(album= "Changes")
Lyrics = rbind(Lyrics1,Lyrics2) %>%
         unnest_tokens(output = word,input = lyric, token = "words") %>%
         inner_join(get_sentiments("nrc") )
#Count the occurrence with each album
Sents = Lyrics %>%group_by(album)%>%count(sentiment)

Sents %>% summarise(sum(n))

Sents %>% filter(!(sentiment =="positive"|sentiment=="negative")) %>%
ggplot(aes(fill=sentiment, y=n, x=album)) + 
    geom_bar(position="fill", stat="identity")
```

# Statistical Test


Consider the Null Hypothesis that Justin Bieber has the same sentiment distribution between 'My World' and 'Changes'.  The alternative is that there is an unspecified difference in distribution.  This can be tested through a Chi-Square test for categorical distributions with $N_r$ rows and $N_c$ columns.
With observed counts $O_{ij}$ in row $i$ and column $j$ of the table, the Expected counts are the data assuming the only difference is the total count; 
\[E_{ij}= (\mbox{row i total*column j total}) / N_{total}\]
This gives the test statistic:

\[X = \sum_{i=1}^{N_r}\sum_{j=1}^{N_c}\frac{(O_{ij}-E_{ij})^2}{E_{ij}}\sim \chi^2_{(N_r-1)*(N_c-1)}\]

*Note1* that we will remove the categories "positive" and "negative" since these are supersets of the other _nrc_ categories.

*Note2* The basic assumption is that these songs are 'bags of words' that Bieber randomly samples into lyrics.  Essentially Bieber has some sentiment distribution at a given time and the lyrics are a random sample thereof.

```{r}
#Cross-tab table:

MyTable = janitor::tabyl(Lyrics%>% filter(!(sentiment =="positive"|sentiment=="negative")) %>%
      group_by(album)   ,sentiment,album)
```


The janitor library makes it easy to build tables that look nice and can format data with row or column percents and counts.

```{r}
MyTable %>% adorn_totals("row")
MyTable %>% adorn_percentages("col")%>%  adorn_ns()

MyTable %>% adorn_percentages("row")
MyTable %>% adorn_percentages("col")
MyTable %>% adorn_percentages("all")%>%  adorn_ns()


#Chi-Square test:
chisq.test(MyTable)
```


There appears to be strong evidence of a change in distribution of sentiments between Justin Bieber Albums.





# Going furhter: Improving the lexicon by considering negation

Using a lexicon, consider the phrase: 
- "my homemade bread is not bad"  
A lexicon based approach would see the word _bad_ and consider the sentence to be negative, whereas _not bad_ is actually pretty good all things considered.

To keep it simple consider just the _bing_ lexicon and first look for negation.
First we will split the lyrics into bigrams, then find places where the first word is a common negation.

The bigram object _Biebgrams_ splits the text into sliding groupings of two word pairs. From there we split apart the bigram and perform sentiment analysis on just the second word.  To get around this we re introduce a NA at the start of each line.  Otherwise the first word of each column (here appearing in column "preword") will be ignored by the sentiment analysis.  Hopefully this makes more sense when you look at the steps.

```{r}
# split into bigrams, but keep the original text to make every step clear
Biebgrams 


Bieber = rbind(Lyrics1,Lyrics2) %>%
       mutate(originalline = lyric) 

# glue NA to the beginning of each string.  The result is that now all of teh original words will be used in the sentiment lexicon replacement.
Bieber$lyric = stringr::str_replace_all(Bieber$lyric, pattern="^", replacement = "NA ")
       

# Split into bigrams, separate apart the bigram into "PreWord" and "word" and perform lexicon sentiment replacement on the second term ("word") while keeping way too many interim steps for illustration:

Bieber   =  Bieber %>%
            unnest_tokens(output = bigram,input = lyric, token = "ngrams", n=2) %>%
            mutate(originalbigram = bigram) %>%
            separate(bigram, c("PreWord", "word"), sep = " ") %>%
            inner_join(get_sentiments("bing") ) 



# find bigrams where the first word might negate the second.
Bieber %>% filter(PreWord %in% c("not","isn't","no"))
# now consider which should be changed.


Bieber[which(Bieber$originalbigram=="no patience"),"sentiment"] = "negative"    
Bieber[which(Bieber$originalbigram=="no approval"),"sentiment"] = "negative"
Bieber[which(Bieber$originalbigram=="not right") ,"sentiment"] = "negative"

Bieber[which(Bieber$originalbigram=="no wrong"),"sentiment"] = "positive"
Bieber[which(Bieber$originalbigram=="not trouble"),"sentiment"] = "positive"

Bieber %>% filter(PreWord %in% c("not","isn't","no"))

```

If the text is expected to have a small number of typos of typos the _amatch_ function from _library(stringdist)_ will look for an approximate match rather than an exact match for strings.



 